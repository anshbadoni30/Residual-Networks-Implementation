**Residual Networks (ResNet) Implementation Project:** <br>
This project implements a Residual Network (ResNet) for image classification tasks. Residual Networks are a type of deep learning architecture that address the vanishing gradient problem in deep neural networks by introducing skip connections, which allow gradients to flow more easily through the network.

**Key components of the project include:**

1. **ResNet50 Model:** The project uses a pre-trained ResNet50 model, which consists of 50 layers and is a powerful tool for image classification tasks. The model is pre-trained on a large dataset and fine-tuned for the specific task.
2. **Evaluation:** The model is evaluated on a test dataset, achieving a high accuracy (~95%). The notebook includes steps to load the pre-trained model, evaluate its performance, and display the results.
3. **Understanding Residual Blocks:** The project emphasizes understanding the importance of skip connections in the architecture, which help in mitigating the vanishing gradient problem, allowing for the training of very deep networks. Two main types of blocks are discussed: the identity block and the convolutional block.
4. **Learning Outcomes:** The project concludes by summarizing the key points, including the significance of skip connections, the types of residual blocks, and the advantages of using ResNet for deep learning tasks.
5. **Resources**: The notebook includes references to the original ResNet paper by He et al. (2015) and the GitHub repository of Fran√ßois Chollet, which provided inspiration for the implementation.
This project serves as a comprehensive guide to understanding and implementing Residual Networks for image recognition tasks, making use of the Keras deep learning framework.
<br>

**Note:** The project does not have resnet50.h5 file
